{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "io_a8t4stK-u",
    "outputId": "f5bd44ed-a9cd-4b06-baab-6a7543da1f8b"
   },
   "outputs": [],
   "source": [
    "import random, math, time\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from math import *\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from matplotlib import rc, animation\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "  from google.colab import files\n",
    "except:\n",
    "  print(\"Could not import Google Colab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzzGQ1u7tOIc"
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "  def __init__(self, grid_size):\n",
    "      self.grid_size = grid_size\n",
    "      \n",
    "      self.cat = imread('https://image.ibb.co/btGeAA/tom.png')\n",
    "      self.mouse = imread('https://image.ibb.co/njNNxq/jerry.png')\n",
    "      self.confetti = imread('https://image.ibb.co/ganuAA/tom-and-jerry.png')\n",
    "      self.dim = 1.5\n",
    "      \n",
    "      self.rewards = []\n",
    "      \n",
    "  def _update_state(self, action):\n",
    "      state = self.state\n",
    "      # 0 = left\n",
    "      # 1 = right\n",
    "      # 2 = down\n",
    "      # 3 = up\n",
    "\n",
    "      fy, fx, py, px = state\n",
    "      old_d = abs(fx - px) + abs(fy - py)\n",
    "\n",
    "      if action == 0:\n",
    "          if px > 0:\n",
    "              px -= 1\n",
    "      if action == 1:\n",
    "          if px < self.grid_size-1:\n",
    "              px += 1\n",
    "      if action == 2:\n",
    "          if py > 0:\n",
    "              py-= 1\n",
    "      if action == 3:\n",
    "          if py < self.grid_size-1:\n",
    "              py += 1\n",
    "\n",
    "      new_d = abs(fx - px) + abs(fy - py)\n",
    "      self.d = old_d-new_d\n",
    "      self.time = self.time - 1\n",
    "      return np.array([fy, fx, py, px])\n",
    "\n",
    "  def _get_reward(self):\n",
    "    fruit_y, fruit_x, player_y, player_x = self.state\n",
    "    if fruit_x == player_x and fruit_y == player_y: return 1\n",
    "    if self.d == 1: return 1\n",
    "    if self.d == 0: return -1\n",
    "    if self.d == -1: return -1\n",
    "\n",
    "  def _is_over(self):\n",
    "    fruit_y, fruit_x, player_y, player_x = self.state\n",
    "    if self.time == 0: return True\n",
    "    if fruit_x == player_x and fruit_y == player_y: return True\n",
    "    return False\n",
    "\n",
    "  def step(self, action):\n",
    "    self.state = self._update_state(action)\n",
    "    reward = self._get_reward()\n",
    "    self.rewards.append(reward)\n",
    "    game_over = self._is_over()\n",
    "    return self.state, reward, game_over\n",
    "  \n",
    "  def render(self):\n",
    "    # Note: there's no promises of efficieny with this method\n",
    "    # If things are slow, remove it\n",
    "    \n",
    "    im_size = (self.grid_size,)*2\n",
    "    state = self.state\n",
    "    \n",
    "    self.fig = plt.figure(figsize=(8, 6), dpi=80)\n",
    "    self.ax = self.fig.add_subplot(111)\n",
    "    \n",
    "    self.ax.clear()\n",
    "    self.ax.set_ylim((-1, self.grid_size))\n",
    "    self.ax.set_xlim((-1, self.grid_size))\n",
    "    #self.ax.axis('off') # uncomment to turn off axes\n",
    "    self.ax.get_xaxis().set_ticks(range(self.grid_size))\n",
    "    self.ax.get_yaxis().set_ticks(range(self.grid_size))\n",
    "    \n",
    "    xc = state[2]\n",
    "    yc = state[3]\n",
    "    xm = state[0]\n",
    "    ym = state[1]\n",
    "    \n",
    "    if state[0] == state[2] and state[1] == state[3]:\n",
    "      self.ax.imshow(self.confetti, \n",
    "                     extent=(-1, self.grid_size,\n",
    "                             -1, self.grid_size))\n",
    "    else:\n",
    "      self.ax.imshow(self.mouse, \n",
    "                     extent=(xm-self.dim/4, xm+self.dim/4,\n",
    "                             ym-self.dim/4, ym+self.dim/4))\n",
    "      self.ax.imshow(self.cat, \n",
    "                     extent=(xc-self.dim/2, xc+self.dim/2,\n",
    "                             yc-self.dim/2, yc+self.dim/2))\n",
    "    self.fig.canvas.draw()\n",
    "    return np.array(self.fig.canvas.renderer._renderer)\n",
    "\n",
    "  def reset(self, deterministic=True):\n",
    "    if deterministic:\n",
    "      # this is an easier environment setup\n",
    "      fruit_x = 0\n",
    "      fruit_y = 0\n",
    "      player_x = self.grid_size - 1\n",
    "      player_y = self.grid_size - 1\n",
    "      time = self.grid_size*2\n",
    "    else:\n",
    "      generated = False\n",
    "      while not generated\\\n",
    "      or abs(fruit_x - player_x) + abs(fruit_y - player_y) < self.grid_size/2:\n",
    "        fruit_x = np.random.randint(0, self.grid_size-1)\n",
    "        fruit_y = np.random.randint(0, self.grid_size-1)\n",
    "        player_x = np.random.randint(0, self.grid_size-1)\n",
    "        player_y = np.random.randint(0, self.grid_size-1)\n",
    "        time = abs(fruit_x - player_x) + abs(fruit_y - player_y)\n",
    "        time *= 2\n",
    "        generated = True\n",
    "\n",
    "    self.time = time\n",
    "    self.d = 0\n",
    "    self.state = np.asarray([fruit_y, fruit_x, player_y, player_x])\n",
    "\n",
    "    return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4028
    },
    "colab_type": "code",
    "id": "KNFJqPD4N3xm",
    "outputId": "df6dd232-84ec-4d15-ed1c-ed16782384e6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This runs the environment using random actions\n",
    "\"\"\"\n",
    "\n",
    "print('Setting up environment')\n",
    "env = Environment(5)\n",
    "num_episodes = 1 # number of games we want the agent to play\n",
    "env.reset()\n",
    "frames = []\n",
    "RENDER = True\n",
    "print('Running random simulation')\n",
    "for episode in range(num_episodes):\n",
    "  print('Resetting environment')\n",
    "  s = env.reset() # Initial state\n",
    "  while True: \n",
    "    a = np.random.choice(range(4)) # choose a random action\n",
    "    s_, r, done = env.step(a) # apply random action\n",
    "    \n",
    "    if RENDER:\n",
    "      fig = env.render()\n",
    "      plt.imshow(fig)\n",
    "      plt.show()\n",
    "      frames.append(fig)\n",
    "\n",
    "    if done:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1czfGeThfWis"
   },
   "outputs": [],
   "source": [
    "#-------------------- BRAIN ---------------------------\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, state_dim, action_dim, weights=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.model = self._createModel()\n",
    "        if weights:\n",
    "          self.model.load_weights(\"brain.h5\")\n",
    "\n",
    "    def _createModel(self):\n",
    "    # Creates a Sequential Keras model\n",
    "    # This acts as the Deep Q-Network (DQN)\n",
    "    \n",
    "        model = Sequential()\n",
    "\n",
    "        first_dense_layer_nodes  = 128 #number of hidden nodes\n",
    "        second_dense_layer_nodes = 4 #number of output nodes\n",
    "        model.add(Dense(first_dense_layer_nodes, input_dim=4))\n",
    "        model.add(Activation('relu')) \n",
    "\n",
    "        model.add(Dense(first_dense_layer_nodes,input_dim=128))\n",
    "        model.add(Activation('relu'))  \n",
    "       \n",
    "        model.add(Dense(second_dense_layer_nodes))\n",
    "        model.add(Activation('linear'))  \n",
    "        model.summary() \n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        opt = RMSprop(lr=0.00025)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        return model\n",
    "    def train(self, x, y, epoch=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=64, nb_epoch=epoch, verbose=verbose)\n",
    "    def predict(self, s):\n",
    "        return self.model.predict(s)\n",
    "\n",
    "    def predictOne(self, s):\n",
    "        return self.predict(s.reshape(1, self.state_dim)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Uq8Aen6fZUD"
   },
   "outputs": [],
   "source": [
    "#-------------------- MEMORY --------------------------\n",
    "class Memory:   \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.samples = []\n",
    "\n",
    "    def add(self, sample):\n",
    "    # a sample should be an array [s, a, r, s_]\n",
    "    # s: current state\n",
    "    # a: current action\n",
    "    # r: current reward\n",
    "    # s_: next state\n",
    "        self.samples.append(sample)        \n",
    "\n",
    "        if len(self.samples) > self.capacity:\n",
    "            self.samples.pop(0)\n",
    "\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nxd6viX_tiCu"
   },
   "outputs": [],
   "source": [
    "#-------------------- AGENT ---------------------------\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_dim, action_dim, memory_capacity = 10000,\n",
    "              batch_size = 64, gamma = 0.99, lamb = 0.001,\n",
    "               max_epsilon = 1., min_epsilon = 0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma # discount rate, to calculate the future discounted reward\n",
    "        self.lamb = lamb\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        self.brain = Brain(state_dim, action_dim)\n",
    "        self.memory = Memory(memory_capacity)\n",
    "        self.steps = 0\n",
    "        self.epsilons = []\n",
    "\n",
    "    def act(self, s, verbose=False):\n",
    "        if random.random() < self.epsilon:\n",
    "            if verbose:\n",
    "                print(\"Random Action.\")\n",
    "            return random.randint(0, self.action_dim-1)\n",
    "        else:\n",
    "            actions = self.brain.predictOne(s)\n",
    "            if verbose:\n",
    "                print(\"Actions:\", actions)\n",
    "            return np.argmax(actions)\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        self.memory.add(sample)        \n",
    "\n",
    "        # slowly decrease Epsilon based on our eperience\n",
    "        self.steps += 1\n",
    "\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        self.epsilon=self.min_epsilon+np.dot((self.max_epsilon-self.min_epsilon),exp(-np.dot(self.lamb,self.steps)))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        self.epsilons.append(self.epsilon)\n",
    "\n",
    "    def replay(self):\n",
    "        # Random sample of experiences\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        batch_size = len(batch)\n",
    "\n",
    "        # Extracting states ('current' and 'next') from samples\n",
    "        no_state = np.zeros(self.state_dim)\n",
    "        states = np.array([ o[0] for o in batch ])\n",
    "        states_next = np.array([ (no_state if o[3] is None else o[3]) for o in batch ])\n",
    "\n",
    "        # Estimating Q-Values for states\n",
    "        q_vals = self.brain.predict(states)\n",
    "        q_vals_next = self.brain.predict(states_next)\n",
    "\n",
    "        # Setting up training data\n",
    "        x = np.zeros((batch_size, self.state_dim))\n",
    "        y = np.zeros((batch_size, self.action_dim))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "              # Observation\n",
    "            obs = batch[i]\n",
    "\n",
    "              # State, Action, Reward, Next State\n",
    "            st = obs[0]; act = obs[1]; rew = obs[2]; st_next = obs[3]\n",
    "            \n",
    "           \n",
    "            \n",
    "\n",
    "              # Estimated Q-Values for Observation\n",
    "            t = q_vals[i]\n",
    "\n",
    "              ### START CODE HERE ### (≈ 4 line of code)\n",
    "            if(self.steps+1==num_episodes):\n",
    "                t[act]=rew\n",
    "            else:\n",
    "                t[act]=rew+np.dot(self.gamma,np.argmax(q_vals_next))\n",
    "              ### END CODE HERE ###\n",
    "\n",
    "              # Set training data\n",
    "            x[i] = st\n",
    "            y[i] = t\n",
    "\n",
    "        # Train\n",
    "        self.brain.train(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53622
    },
    "colab_type": "code",
    "id": "eNF00A8ktvUs",
    "outputId": "3dd987dc-edff-4409-eafa-0527439cf298"
   },
   "outputs": [],
   "source": [
    "#-------------------- MAIN ----------------------------\n",
    "print('Setting up environment')\n",
    "env = Environment(5)\n",
    "\n",
    "state_dim = 4\n",
    "action_dim = 4 # left, right, up, down\n",
    "print('Setting up agent')\n",
    "MAX_EPSILON = 1 # the rate in which an agent randomly decides its action\n",
    "MIN_EPSILON = 0.05 # min rate in which an agent randomly decides its action\n",
    "LAMBDA = 0.00005      # speed of decay for epsilon\n",
    "num_episodes = 100 # number of games we want the agent to play\n",
    "\n",
    "VERBOSE = False\n",
    "agent = Agent(state_dim, action_dim, lamb=LAMBDA,\n",
    "              max_epsilon=MAX_EPSILON, min_epsilon=MIN_EPSILON)\n",
    "env.reset()\n",
    "episode_rewards = []\n",
    "epsilons = []\n",
    "t0 = time.time()\n",
    "frames = []\n",
    "\n",
    "print('Running simulation')\n",
    "for episode in range(num_episodes):\n",
    "  \n",
    "  s = env.reset() # Initial state\n",
    "  if episode % 1000 == 0:\n",
    "      fig = env.render()\n",
    "      frames.append(fig)\n",
    "  \n",
    "  R = 0\n",
    "  while True: \n",
    "    a = agent.act(s, verbose=VERBOSE)\n",
    "\n",
    "    s_, r, done = env.step(a)\n",
    "\n",
    "    if done: # terminal state\n",
    "        s_ = None\n",
    "\n",
    "    agent.observe( (s, a, r, s_) )\n",
    "    agent.replay()\n",
    "\n",
    "    s = s_\n",
    "    R += r\n",
    "    \n",
    "    if episode % 1000 == 0:\n",
    "      fig = env.render()\n",
    "      frames.append(fig)\n",
    "    \n",
    "    if VERBOSE:\n",
    "      print(\"Action:\", a)\n",
    "      print(\"Reward:\", r)\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "      \n",
    "  epsilons.append(agent.epsilon)\n",
    "  episode_rewards.append(R)\n",
    "  \n",
    "  if episode % 100 == 0:\n",
    "    print('Episode {}'.format(episode))\n",
    "    print('Time Elapsed: {0:.2f}s'.format(time.time() - t0))\n",
    "    print('Epsilon {}'.format(epsilons[-1]))\n",
    "    print('Last Episode Reward: {}'.format(R))\n",
    "    print('Episode Reward Rolling Mean: {}'.format(np.mean(episode_rewards[:-100])))\n",
    "    print('-'*10)\n",
    "\n",
    "agent.brain.model.save(\"brain.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "eBiUaA2vSipn",
    "outputId": "76b3d57d-0a7d-4a0b-e3ab-569498735ae7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.title(\"Epsilon\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Epsilon value\")\n",
    "plt.plot(epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "9xjl4YwpS8WU",
    "outputId": "b136623e-cb56-429c-a08f-9682559a3183"
   },
   "outputs": [],
   "source": [
    "smoothing = 50\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.title(\"Episode Reward\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"{} MA Reward\".format(smoothing))\n",
    "episode_ma = np.convolve(episode_rewards, \n",
    "                         np.ones((smoothing,))/smoothing, mode='valid')\n",
    "plt.plot(episode_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 11988
    },
    "colab_type": "code",
    "id": "6I0iVz-MtXG-",
    "outputId": "ced313a8-8507-420a-ba73-ee5cef98890a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "To create animation, you need to install ffmpeg\n",
    "These lines will install it in Google Colab\n",
    "If you're running this notebook locally, you'll need to install\n",
    "ffmpeg on your computer: \n",
    "https://github.com/adaptlearning/adapt_authoring/wiki/Installing-FFmpeg\n",
    "\n",
    "Note: this lines will only work in Google Colab, do not run them locally.\n",
    "\"\"\"\n",
    "\n",
    "!apt install ffmpeg\n",
    "!which ffmpeg\n",
    "plt.rcParams['animation.ffmpeg_path'] = u'/usr/bin/ffmpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "E9ovkf9LMOZE",
    "outputId": "8e4b591a-e351-4a3b-ecd3-ee723f2f83f3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell will compile the frames that should have been saved during training\n",
    "into an animation.  This required ffmpeg to be installed.\n",
    "\n",
    "If the main portion wasn't modified, this will have saved frames from every\n",
    "1,000 episode.  In the animation, you should see it start off performing poorly,\n",
    "but as it progresses it should perform optimally.\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.axis('off')\n",
    "l = ax.imshow(frames[0])\n",
    "\n",
    "def animate(i):\n",
    "    l.set_data(frames[i])\n",
    "\n",
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=12, metadata=dict(artist='Me'))\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(frames))\n",
    "\n",
    "ani.save('animation.mp4', writer=writer, dpi=220)\n",
    "time.sleep(5) # let it process (only necessary in Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xLRv4G5JB0y"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you're using Google Colab, you'll need to use the Google Colab\n",
    "download function to download both the model you trained and the animation\n",
    "you created.\n",
    "\n",
    "If you're not using Google Colab, those files should be saved in the directory\n",
    "where this notebook is located.\n",
    "\"\"\"\n",
    "\n",
    "# To Save Brain\n",
    "files.download(\"brain.h5\")\n",
    "\n",
    "# To Save Animation\n",
    "files.download('animation.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEVjv7A-HFPq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RL_Project_4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
